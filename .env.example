# =============================================================================
# ENVIRONMENT CONFIGURATION
# =============================================================================
# Copy to .env and modify for your environment

# -----------------------------------------------------------------------------
# DEPLOYMENT TARGET (uncomment one)
# -----------------------------------------------------------------------------
# For MacBook Pro M3 MAX development/testing (CPU only)
DEPLOYMENT_TARGET=development
DEVICE=cpu
TORCH_DTYPE=float32
WHISPER_MODEL=medium
MAX_AUDIO_DURATION=1800  # 30 min limit for CPU processing
MAX_FILE_SIZE=50000000   # 50MB

# For Ubuntu production server (RTX 4090, 32GB RAM)
# DEPLOYMENT_TARGET=production
# DEVICE=cuda
# TORCH_DTYPE=float16
# WHISPER_MODEL=large-v3
# MAX_AUDIO_DURATION=7200  # 2 hours
# MAX_FILE_SIZE=100000000  # 100MB

# -----------------------------------------------------------------------------
# REDIS CONFIGURATION
# -----------------------------------------------------------------------------
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0

# -----------------------------------------------------------------------------
# HUGGINGFACE AUTHENTICATION
# -----------------------------------------------------------------------------
# Required for pyannote models - get token from https://hf.co/settings/tokens
# Also accept terms at https://hf.co/pyannote/speaker-diarization-3.1
HUGGINGFACE_TOKEN=your_token_here

# -----------------------------------------------------------------------------
# MODEL CONFIGURATION
# -----------------------------------------------------------------------------
DIARIZATION_MODEL=pyannote/speaker-diarization-3.1

# -----------------------------------------------------------------------------
# PROCESSING LIMITS
# -----------------------------------------------------------------------------
UPLOAD_DIR=uploads

# -----------------------------------------------------------------------------
# OLLAMA INTEGRATION
# -----------------------------------------------------------------------------
# Ollama server is available on beefybox for both dev and prod
OLLAMA_ENABLED=true
OLLAMA_HOST=http://beefybox.lan:11434
OLLAMA_MODEL=llama3.2

# To disable Ollama (uncomment to disable)
# OLLAMA_ENABLED=false

# -----------------------------------------------------------------------------
# PERFORMANCE TUNING (Production Ubuntu only)
# -----------------------------------------------------------------------------
# Uncomment for production Ubuntu deployment
# OMP_NUM_THREADS=16
# MKL_NUM_THREADS=16
# CUDA_VISIBLE_DEVICES=0